# Relation Extraction with Pre-training
This project is about relation extraction adopting two novel pre-training methods for better performance in data-scarce setting.
Before supervised learning with relatively a small amount of labeled data, unsupervised pre-training is performed with a large amount of text data.
Inspired by skip-gram model, the model attempts to predict key words, defined in two different ways, from sentence embedding during pre-training phase.

## Built With
* [Python2.7](https://www.python.org/download/releases/2.7/)
* [Tensorflow](https://www.tensorflow.org/)


## References
* Wang, Yao, Wan‐dong Cai, and Peng‐cheng Wei. "A deep learning approach for detecting malicious JavaScript code." Security and Communication Networks 9.11 (2016): 1520-1534.
* Zhang, Xiang, Junbo Zhao, and Yann LeCun. "Character-level convolutional networks for text classification." Advances in neural information processing systems. 2015.
